{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c23cd75",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border: none;\">\n",
    "    <tr>\n",
    "        <td colspan=\"3\" style=\"text-align:center; border: none;\">\n",
    "            <img src=\"assets/banner.svg\" alt=\"Banner Image\" style=\"width:100%;\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <!-- Add other rows and cells below if needed -->\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb7e8e",
   "metadata": {},
   "source": [
    "This Notebook provides a complete workflow for downloading, processing, analyzing, and visualizing temperature data for various capital cities. Here's a step-by-step explanation of what each part of the code does:\n",
    "\n",
    "1. **Loading Configuration**: The function `load_capitals_coordinates` reads a YAML configuration file to get the coordinates (latitude and longitude) of different capital cities.\n",
    "\n",
    "2. **Downloading the Dataset**: The `get_cacheB_dataset` function downloads data from a specified Dataset URL. It uses the xarray library to handle the dataset efficiently.\n",
    "\n",
    "3. **Preprocessing the Data**: The `preprocess` function extracts temperature data for a specific city, averages it over desire period, converts the temperature from Kelvin to Celsius and load the data in the machine memory.\n",
    "\n",
    "4. **Basic Plotting**: The `basic_plot` function creates a simple line plot of the daily average temperature for a given city.\n",
    "\n",
    "5. **Training a Forecast Model**: The `train_model` function prepares the data and trains a forecasting model using the Prophet library. It splits the data into training and testing sets [80%-20%]and fits the model to the training data. For more information please checkout the following link: [Prophet](https://facebook.github.io/prophet/docs/quick_start.html)\n",
    "\n",
    "\n",
    "6. **Making Predictions**: The `make_predictions` function uses the trained model to make temperature predictions on the test data and calculates error metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).\n",
    "\n",
    "7. **Plotting Forecasts**: The `plot_forecast` function plots the training data, test data, and forecasted temperatures, allowing you to visually compare the model's predictions with the actual temperatures. It also provides options to display the plot and save it as an SVG file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (load_capitals_coordinates, get_cacheB_dataset,\n",
    "                   basic_plot, make_predictions,\n",
    "                   plot_forecast, preprocess, train_model)\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load capital coordinates from the YAML file\n",
    "capitals_coordinates = load_capitals_coordinates('config.yaml')\n",
    "# Create and display the dropdown\n",
    "# Variable to store the selected coordinates\n",
    "def display_coordinates(city):\n",
    "\n",
    "    global selected_coordinates\n",
    "    global selected_city\n",
    "    selected_coordinates = capitals_coordinates[city]\n",
    "    selected_city = city\n",
    "    return selected_coordinates, selected_city\n",
    "\n",
    "# Create and display the dropdown\n",
    "widgets.interact(display_coordinates, city=sorted(list(capitals_coordinates.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Start and end dates\n",
    "start_date = datetime.strptime(\"20200101\", \"%Y%m%d\")\n",
    "end_date = datetime.strptime(\"20200110\", \"%Y%m%d\")\n",
    "# Generate dates for the whole year with a daily timestep\n",
    "current_date = start_date\n",
    "date = \"\"\n",
    "while current_date <= end_date:\n",
    "    # Update the \"date\" field in the request dictionary\n",
    "    # polytope_request[\"date\"] = current_date.strftime(\"%Y%m%d\")\n",
    "    # data = earthkit.data.from_source(\"polytope\", \"destination-earth\", polytope_request, address=polytope_url, stream=False)\n",
    "    # # Process the request (e.g., print it, send it to an API, etc.)\n",
    "    tmp_date = current_date.strftime(\"%Y%m%d\")\n",
    "    date = f\"{date}/{tmp_date}\"\n",
    "    # print(date)\n",
    "    # # Move to the next day\n",
    "    current_date += timedelta(days=1)\n",
    "date=date[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564e17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open(\"config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "polytope_url = config[\"polytope_url\"]\n",
    "polytope_request = config[\"polytope_request\"]\n",
    "grid = config['grid']\n",
    "# polytope_request[\"date\"] = date\n",
    "polytope_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17689083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthkit.data\n",
    "import earthkit.regrid\n",
    "URL_DATASET = \"polytope.lumi.apps.dte.destination-earth.eu\"\n",
    "data = earthkit.data.from_source(\"polytope\", \"destination-earth\", polytope_request, address=polytope_url, stream=False)\n",
    "out_grid = {\"grid\": [grid['lat'], grid['lon']]}\n",
    "data_latlon = earthkit.regrid.interpolate(data, out_grid=out_grid, method=grid['method'])\n",
    "ds = data_latlon.to_xarray()\n",
    "ds = ds[\"t2m\"]\n",
    "dataset = ds.sel(latitude=selected_coordinates[0], longitude=selected_coordinates[1], method=\"nearest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset = dataset.resample(time=\"D\").mean(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "index = dataset.time\n",
    "df = pd.DataFrame(data={\"time\": index,\n",
    "                        \"temperature\": dataset.values.flatten()})\n",
    "df[\"temperature\"] = df[\"temperature\"] - 273\n",
    "basic_plot(df, city=selected_city, coord=selected_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137a2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "# Load the configuration\n",
    "with open(\"config.yaml\", \"r\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "polytope_url = config[\"polytope_url\"]\n",
    "polytope_request = config[\"polytope_request\"]\n",
    "grid = config['grid']\n",
    "\n",
    "# Generate list of dates for N days\n",
    "N = 30  # Number of days\n",
    "start_date = pd.to_datetime(polytope_request[\"date\"])  # Assume the start date is provided in the config\n",
    "dates = [start_date + pd.Timedelta(days=i) for i in range(N)]\n",
    "\n",
    "# Initialize an empty list to store datasets\n",
    "datasets = []\n",
    "\n",
    "for date in dates:\n",
    "    # Modify the polytope_request for the current date\n",
    "    polytope_request[\"date\"] = date.strftime(\"%Y%m%d\")\n",
    "\n",
    "    # Query the data\n",
    "    data = earthkit.data.from_source(\"polytope\", \"destination-earth\", polytope_request, address=polytope_url, stream=False)\n",
    "    out_grid = {\"grid\": [grid['lat'], grid['lon']]}\n",
    "    data_latlon = earthkit.regrid.interpolate(data, out_grid=out_grid, method=grid['method'])\n",
    "\n",
    "    # Convert to xarray\n",
    "    ds = data_latlon.to_xarray()\n",
    "    ds = ds[\"t2m\"]\n",
    "\n",
    "    # Select the nearest point\n",
    "    selected_ds = ds.sel(latitude=selected_coordinates[0], longitude=selected_coordinates[1], method=\"nearest\")\n",
    "\n",
    "    # Add the dataset to the list\n",
    "    datasets.append(selected_ds)\n",
    "\n",
    "# Concatenate all datasets along a new dimension 'time'\n",
    "final_dataset = xr.concat(datasets, dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60dd796",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = final_dataset.time\n",
    "df = pd.DataFrame(data={\"time\": index,\n",
    "                        \"temperature\": final_dataset.values.flatten()})\n",
    "df[\"temperature\"] = df[\"temperature\"] - 273\n",
    "basic_plot(df, city=selected_city, coord=selected_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a081fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
